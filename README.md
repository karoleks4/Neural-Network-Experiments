# Neural-Network-Experiments

Reports including experiments on different neural network architectures for one of the Machine Learning courses (Python, Tensorflow):

**Assignment 1**: 
Experiments on MNIST dataset involving:
* Different activation functions (ReLu, *Leaky* ReLu, ELU, SELU) 
* Various initialisation strategies (Glorot, SELUInit)  
* Number of hidden layers

**Assignment 2**: 
Continuation of Assignemnt 1, evaluated on Balanced MNIST dataset involving:
* Own implementation of batch normalisation and convolutional layer
* Different learning rules (SGD, RMSProp, Adam)
* Various configurations of batch normalisation and ConvNets depth
* Performance comparison before best performing DNN and CNN

**Assignment 3**: 
Intermin report for group project of own choice (i.e. study and experiments on Deep-Fakes GANs):
* Overview of GANs, Image-to-Image translation and face swapping techniques
* Dataset generation: Trump-Clinton, Author-Author datasets
* Comparison of different training set-ups on final results

**Assignment 4**: 
Continuation of Assignment 3 (i.e. study and experiments on Deep-Fakes GANs). Experiments involving:
* Batch normalisation
* PixelShuffle layer
* XGAN
* Frequent descriminator updates
* Different input sizes
