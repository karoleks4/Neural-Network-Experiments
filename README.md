# Neural-Network-Experiments

Reports including experiments on different neural network architectures for one of the Machine Learning courses (Python, Tensorflow):

**Assignment 1**: 
Experiments on MNIST dataset involving:
* Different activation functions (ReLu, *Leaky* ReLu, ELU, SELU) 
* Various initialisation strategies (Glorot, SELUInit)  
* Number of hidden layers

**Assignment 2**: 
Continuation of Assignemnt 1, evaluated on Balanced MNIST dataset involving:
* Own implementation of batch normalisation and convolutional layer
* Different learning rules (SGD, RMSProp, Adam)
* Various configurations of batch normalisation and ConvNets depth
* Performance comparison before best performing DNN and CNN

**Assignment 3**: 
